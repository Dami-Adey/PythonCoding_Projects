{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgRn+FMH+JAhXYG0JqX4IS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dami-Adey/Manual_NN_Dense/blob/main/NN_manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY56k7IVWfPg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Layer Object"
      ],
      "metadata": {
        "id": "jjlAPoxQimtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_DenseLayer():\n",
        "  # class variables for scaling initial weights/biases\n",
        "  WB_SCALE = 0.1\n",
        "\n",
        "  def __init__(self,units,activation='sigmoid',name=None,W_init=None,B_init=None):\n",
        "    # initialise layer data\n",
        "    self.neurons = units          # number of neurons in the layer\n",
        "    self.activation = activation  # activation function applied at the layer\n",
        "    self.name = name              # layer name (if desired)\n",
        "\n",
        "    # features will be updated when an input is passed to the layer\n",
        "    self.features = None\n",
        "\n",
        "    # initialise layer weights and biases (inactive by default)\n",
        "    self.wb_init = False\n",
        "    self.W = W_init\n",
        "    self.B = B_init\n",
        "\n",
        "    # input sanitation parameter\n",
        "    self.expectedInputShape = None\n",
        "    \n",
        "  \n",
        "  def __str__(self):\n",
        "    '''\n",
        "      Layer print protocol.\n",
        "    '''\n",
        "    # print dialogue\n",
        "    return f\"\\nNeural-network layer (dense) with {self.neurons} neurons. {self.activation.title()} activation. \\n\"\n",
        "\n",
        "  \n",
        "  def initialiseWB(self,A_in,featureCount,neuronCount):\n",
        "    ''' \n",
        "    Estimates initial values for weight and bias arrays:\n",
        "\n",
        "    WEIGHT INITIALISATION PROTOCOL:\n",
        "    --------------------------------------------------------------------------\n",
        "      - the total number of elements in the weight matrix, W_elem = n*j\n",
        "      - create an array with elements totaling W_elem\n",
        "      - then reshape array to the desired (n,j) and scale the values using WB_INIT\n",
        "    \n",
        "    BIAS INITIALISATION PROTOCOL:\n",
        "    --------------------------------------------------------------------------\n",
        "      - the total number of elements in the bias vector, B_elem = j\n",
        "      - create an array with elements totaling B_elem\n",
        "      - reshape to the desired (j,) and scale the values using WB_INIT\n",
        "    '''\n",
        "    # find total elements in weight matrix\n",
        "    W_elem = featureCount*neuronCount  # n*j\n",
        "    # reshape weight array, and scale\n",
        "    weight_arr = self.WB_SCALE*np.arange(1,W_elem+1,1).reshape(featureCount,neuronCount)\n",
        "    \n",
        "    # total bias vector elements\n",
        "    B_elem = neuronCount  # j\n",
        "    # reshape bias array, and scale\n",
        "    bias_arr = self.WB_SCALE*np.arange(1,B_elem+1,1).reshape(neuronCount,)\n",
        "    # NOTE^: a 1D vector is fine for B since python will automatically 'brodacast'\n",
        "    # the values correctly to align with the shape of A_in*W. \n",
        "\n",
        "    return weight_arr, bias_arr \n",
        "\n",
        "  \n",
        "  def is_consistent(self,A_in):\n",
        "    '''\n",
        "    Checks if the input A_in is consistent with previous inputs to the layer.\n",
        "    '''\n",
        "    # returns true if the input shape is consistent with the previous iterations\n",
        "    if A_in.shape == self.expectedInputShape:\n",
        "      return True\n",
        "    # if a new input format is detected, update the expected input shape to match the new input\n",
        "    self.expectedInputShape = A_in.shape\n",
        "    return False\n",
        "  \n",
        "  \n",
        "  def count_features(self,A_in):\n",
        "    '''\n",
        "    Counts the number of features, n, present in the input A_in\n",
        "    '''\n",
        "    try:\n",
        "      # if A_in is a matrix, n is the number of columns, index 1\n",
        "      numFeatures = A_in.shape[1]       \n",
        "    except IndexError:\n",
        "      # an IndexError above would suggest that A_in is a 1D array. Therefore n is index 0\n",
        "      numFeatures = A_in.shape[0]\n",
        "\n",
        "    return numFeatures\n",
        "\n",
        "  \n",
        "  def activate(self, A_in):\n",
        "    '''\n",
        "      Evaluates the layer output(s) for an input A_in:\n",
        "\n",
        "      A_in -  input data    (m,n)  |  m examples with n features each\n",
        "      W    -  layer weights (n,j)  |  n features per neuron, j neurons/units\n",
        "      B    -  bias vector   (1,j)  |  j neurons/units\n",
        "    '''\n",
        "    self.current_input = A_in\n",
        "    # check if the layer was expecting an input of this format\n",
        "    if self.is_consistent(self.current_input):\n",
        "      pass # do nothing if input was expected in this format\n",
        "    else:\n",
        "      # count the number of features in the new input format\n",
        "      self.features = self.count_features(self.current_input)\n",
        "      # initialise weight/bias arrays for the new input\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      # indicate that weights/biases have been initialised\n",
        "      self.wb_init = True \n",
        "             \n",
        "    # if weights/biases are not initialised, then estimate them.\n",
        "    if not self.wb_init:\n",
        "      # estimate inital values for W and B\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons) \n",
        "      # the layer is now initialised\n",
        "      self.wb_init = True\n",
        "    \n",
        "    # apply weights an biases to inputs; (z = A_in*W + B)\n",
        "    z = np.matmul(self.current_input, self.W) + self.B\n",
        "\n",
        "    # apply the chosen activation function\n",
        "    # 1.0 sigmoid\n",
        "    if self.activation.lower() == 'linear':\n",
        "      self.out = self.linear(z)\n",
        "    elif self.activation.lower() == 'sigmoid':\n",
        "      self.out = self.sigmoid(z)\n",
        "    elif self.activation.lower() == 'relu':\n",
        "      self.out = self.relu(z)\n",
        "    else:\n",
        "      self.out = None\n",
        "      raise Exception(\"Invalid activation function.\")\n",
        "\n",
        "    return self.out\n",
        "\n",
        "  \n",
        "  def get_weights(self):\n",
        "    '''\n",
        "      Returns the current weights and biases for the layer\n",
        "    '''\n",
        "    return [self.W, self.B]\n",
        "\n",
        "  \n",
        "  def set_weights(self,W_set,B_set):\n",
        "    '''\n",
        "      Overwrites the weights and biases of the current layer\n",
        "    '''\n",
        "    try:\n",
        "      # if setpoint values are null, then W/B are set to None\n",
        "      if (W_set == None) and (B_set == None):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "        # indicate that the weights/biases have been nullified\n",
        "        self.wb_init = False\n",
        "    \n",
        "    except ValueError:\n",
        "      if (self.W == None) or (self.B == None):\n",
        "        # creat proxy weights/biases using the current input structure\n",
        "        W_proxy, B_proxy = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      else:\n",
        "        W_proxy = self.W\n",
        "        B_proxy = self.B\n",
        "      \n",
        "      # if non-null, the shape of the weight and bias structures should be consistent\n",
        "      if (W_set.shape == W_proxy.shape) and (B_set.shape == B_proxy.shape):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "      # if not, the W/B structures chosen are incompatible  \n",
        "      else:\n",
        "        return print(f\"\\nINVALID COMMAND(!): \\n\\nLayer is only compatible with a {W_proxy.shape} weight matrix, and {B_proxy.shape} bias vector\")\n",
        "\n",
        "  \n",
        "  #----------------------------------------------------------------------------#\n",
        "  # ACTIVATION FUNCTIONS\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # Linear function (regression problems; negative or positive)\n",
        "  def linear(self,z):\n",
        "    '''\n",
        "      Evaluates the linear function g on an input z\n",
        "\n",
        "      Such that, g(z) = z\n",
        "    '''\n",
        "    return z\n",
        "  \n",
        "  # Sigmoid function (binary classification problems; 1 or 0, etc.)\n",
        "  def sigmoid(self,z):\n",
        "    '''\n",
        "      Evaluates the sigmoid function g on an input z\n",
        "\n",
        "      Where, g(z) =      1\n",
        "                     ----------\n",
        "                     1 + e^(-z)\n",
        "    '''\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "  # ReLU function (regression problems; threshold utility; non-negative values only)\n",
        "  def relu(self,z):\n",
        "    '''\n",
        "      Evaluates the Rectified Linear Unit (ReLU) function g on an input z\n",
        "\n",
        "      Where, g(z) = 0, if (z < 0)\n",
        "             g(z) = z, if (z > 0)\n",
        "    '''\n",
        "    return max(0,z)\n",
        "\n",
        "  # Softmax activation (multi-class classification, i.e. mutiple dicrete choices, one correct choice per example)\n",
        "  def softmax(self,z):\n",
        "    '''\n",
        "    Evaluates the softmax activation function g on an input z. Softmax converts\n",
        "    z for all possible choice into a distribution of probabilities that each\n",
        "    choice is correct.\n",
        "\n",
        "    For a problem with N choices, the probability that the kth choice is \n",
        "    correct is represented by:\n",
        "\n",
        "           g(z_{i,j,k,...,N}) =         e^z_k\n",
        "                                 -------------------\n",
        "                                 e^z_i + ... + e^z_N\n",
        "    '''\n",
        "    e_z = np.exp(z)\n",
        "    return e_z/np.sum(e_z)\n"
      ],
      "metadata": {
        "id": "Z5D0zYRqXQix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests - Layer Behaviour"
      ],
      "metadata": {
        "id": "sXRMB-A7YJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new dense layer\n",
        "L1 = NN_DenseLayer(3,'Sigmoid','layer1')"
      ],
      "metadata": {
        "id": "5_dkXBaq2Eva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new layer\n",
        "print(L1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGwoTaaJy8m",
        "outputId": "dc24e500-08a4-43c1-c6fe-2fc5270e861e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvddf9FfJy4e",
        "outputId": "b0c7f81d-eb5d-4f4f-c22d-1b139047a6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the expected input shape\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "id": "gSWzbGtuhv0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate the layer by passing an array input\n",
        "test_in = np.array([1,2,3])\n",
        "L1.activate(test_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRdiShgJywe",
        "outputId": "8653ff17-1c92-4c92-91b5-00c7aac809f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95689275, 0.97811873, 0.98901306])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input shape should now match the most recent input (1D array)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqOo470Qh3ch",
        "outputId": "369e793f-beef-4f39-c117-ff5603bbe96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBq34AF2Y3wp",
        "outputId": "79cd7170-b4a0-4eeb-cec3-02b452b65190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.1, 0.2, 0.3],\n",
              "        [0.4, 0.5, 0.6],\n",
              "        [0.7, 0.8, 0.9]]),\n",
              " array([0.1, 0.2, 0.3])]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset weights\n",
        "W = None\n",
        "B = None\n",
        "L1.set_weights(W, B)\n",
        "L1.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPVVEb1ZFWt",
        "outputId": "f6a2fc4d-fe1a-4591-bbf3-a5fe3bddf4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set invalid weights\n",
        "W = np.arange(1,3,1)\n",
        "B = np.arange(1,10,2)\n",
        "L1.set_weights(W, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5OBmD8Zuad",
        "outputId": "a8515fbd-3f8b-4d9d-b96e-ded6d65f9a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INVALID COMMAND(!): \n",
            "\n",
            "Layer is only compatible with a (3, 3) weight matrix, and (3,) bias vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that layer weights were NOT changed\n",
        "L1.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZVmLEXpgtvR",
        "outputId": "f0123e2c-2ba8-4bbc-8f75-c469d2b618bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activate for a new input (matrix type)\n",
        "new_in = np.array([[1,2,3],[4,5,6]])\n",
        "L1.activate(new_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdCd89_cgwSy",
        "outputId": "662b988b-5f0e-466e-9e69-fbbc2f32d533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.95689275, 0.97811873, 0.98901306],\n",
              "       [0.9987706 , 0.99975154, 0.99994983]])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e_1u2QhDOS",
        "outputId": "1c105dfa-06f7-4873-b7b8-0f6ff891c5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.1, 0.2, 0.3],\n",
              "        [0.4, 0.5, 0.6],\n",
              "        [0.7, 0.8, 0.9]]),\n",
              " array([0.1, 0.2, 0.3])]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input should update to match the new input type (2D matrix)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvaNUJ1hOL7",
        "outputId": "c19c88b1-fcd0-4f75-823f-1eb66ba896b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential NN Compiler"
      ],
      "metadata": {
        "id": "Kw-guNhXiujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Sequential:\n",
        "  def __init__(self,layers):\n",
        "    # assign layer identifiers for the NN\n",
        "    self.layers = layers\n",
        "    self.current_input = None\n",
        "\n",
        "  \n",
        "  def summarise(self):\n",
        "    print(f\"{len(self.layers)} Layer Neural-Network (Sequential):\")\n",
        "    print(f\"------------------------------------\\n\")\n",
        "    for id in range(len(self.layers)):\n",
        "      print(f\"Layer {id + 1}\")\n",
        "      print(self.layers[id])\n",
        "\n",
        "  \n",
        "  def compile(self):\n",
        "    pass\n",
        "\n",
        "  \n",
        "  def fit(self):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "8D2pILctiZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests - Sequential NN Behaviour"
      ],
      "metadata": {
        "id": "1Th5Qq6Dmfq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create NN layer objects\n",
        "L1 = NN_DenseLayer(25,'sigmoid','layer1')\n",
        "L2 = NN_DenseLayer(15,'sigmoid','layer2')\n",
        "L3 = NN_DenseLayer(1,'sigmoid','layer3')\n",
        "\n",
        "# pass layers to NN sequantial constructor model\n",
        "model = NN_Sequential([L1,L2,L3])"
      ],
      "metadata": {
        "id": "pHRMZhZ4icuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model.summarise()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXlogTcimlsI",
        "outputId": "a4fded27-7b16-4795-af21-b1fc746f361c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 Layer Neural-Network (Sequential):\n",
            "------------------------------------\n",
            "\n",
            "Layer 1\n",
            "\n",
            "Neural-network layer (dense) with 25 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 2\n",
            "\n",
            "Neural-network layer (dense) with 15 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 3\n",
            "\n",
            "Neural-network layer (dense) with 1 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.array([-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "id": "zEovW9Bgmr_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = [];\n",
        "for num in test:\n",
        "  test2.append(max(0,num))\n",
        "print(test2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UirbPkyAD60L",
        "outputId": "c310d3ba-a6ca-44b3-b15a-52106f315caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(0,test.any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wey918PBEGPq",
        "outputId": "b0423a96-097d-4598-a039-945d4efec00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7dFt8qSK4Jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}